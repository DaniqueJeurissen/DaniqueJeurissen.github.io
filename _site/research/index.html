<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<head>
<meta charset="utf-8">
<title>Perceptual decision making &#8211; Danique Jeurissen</title>
<meta name="description" content="“Research”">
<meta name="keywords" content="">



<!-- Twitter Cards -->
<meta name="twitter:title" content="Perceptual decision making">
<meta name="twitter:description" content="“Research”">
<meta name="twitter:site" content="@d_jeurissen_">
<meta name="twitter:creator" content="@d_jeurissen_">

<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://daniquejeurissen.github.io/images/Background.jpg">

<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Perceptual decision making">
<meta property="og:description" content="“Research”">
<meta property="og:url" content="https://daniquejeurissen.github.io/research/">
<meta property="og:site_name" content="Danique Jeurissen">





<link rel="canonical" href="https://daniquejeurissen.github.io/research/">
<link href="https://daniquejeurissen.github.io/feed.xml" type="application/atom+xml" rel="alternate" title="Danique Jeurissen Feed">


<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="https://daniquejeurissen.github.io/assets/css/main.css">

<meta http-equiv="cleartype" content="on">

<!-- HTML5 Shiv and Media Query Support -->
<!--[if lt IE 9]>
	<script src="https://daniquejeurissen.github.io/assets/js/vendor/html5shiv.min.js"></script>
	<script src="https://daniquejeurissen.github.io/assets/js/vendor/respond.min.js"></script>
<![endif]-->

<!-- Modernizr -->
<script src="https://daniquejeurissen.github.io/assets/js/vendor/modernizr-2.7.1.custom.min.js"></script>

<link href='//fonts.googleapis.com/css?family=PT+Sans+Narrow:400,700%7CPT+Serif:400,700,400italic' rel='stylesheet' type='text/css'>

<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="https://daniquejeurissen.github.io/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="https://daniquejeurissen.github.io/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="https://daniquejeurissen.github.io/images/apple-touch-icon-precomposed.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="https://daniquejeurissen.github.io/images/apple-touch-icon-72x72-precomposed.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="https://daniquejeurissen.github.io/images/apple-touch-icon-114x114-precomposed.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://daniquejeurissen.github.io/images/apple-touch-icon-144x144-precomposed.png">

</head>

<body class="page">

<!--[if lt IE 9]><div class="browser-upgrade alert alert-info">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div><![endif]-->

<div class="navigation-wrapper">
	<div class="site-name">
		<a href="https://daniquejeurissen.github.io">Danique Jeurissen</a>
	</div><!-- /.site-name -->
	<div class="top-navigation">
		<nav role="navigation" id="site-nav" class="nav">
		    <ul>
		        
					    
					        
					    
					    <li><a href="https://daniquejeurissen.github.io/research/" >Research</a></li>
					  
					    
					        
					    
					    <li><a href="https://daniquejeurissen.github.io/publications/" >Publications</a></li>
					  
					    
					        
					    
					    <li><a href="https://daniquejeurissen.github.io/cv/" >Curriculum Vitae</a></li>
					  
					    
					        
					    
					    <li><a href="https://daniquejeurissen.github.io/join/" >Join</a></li>
					  
					    
					        
					    
					    <li><a href="https://daniquejeurissen.github.io/contact/" >Contact</a></li>
					  
		    </ul>
		</nav>
	</div><!-- /.top-navigation -->
</div><!-- /.navigation-wrapper -->


  <div class="image-wrap">
  <img src=
    
      "https://daniquejeurissen.github.io/images/Background.jpg"
    
  alt="Perceptual decision making feature image">
  
  </div><!-- /.image-wrap -->


<div id="main" role="main">
  <div class="article-author-side">
    



	<img src="https://daniquejeurissen.github.io/images/danique.jpg" class="bio-photo" alt="Danique Jeurissen bio photo">

<h3>Danique Jeurissen</h3>
<p>Neuroscientist</p>
<a href="mailto:d.jeurissen@columbia.edu" class="author-social" target="_blank"><i class="fa fa-fw fa-envelope-square"></i> Email</a>
<a href="http://twitter.com/d_jeurissen_" class="author-social" target="_blank"><i class="fa fa-fw fa-twitter-square"></i> Twitter</a>














  </div>
  <article class="page">
    <h1>Perceptual decision making</h1>
    <div class="article-wrap">
      <p>When we make daily-life decisions, we can rely on any source of information. For example, when driving up to an intersection, we use information about the color of the traffic light to decide whether to cross. Just seconds later, we may use information about the motion of approaching cars to decide whether it is safe to make a left turn. How do we flexibly select a source of information for a choice?<br /><br /></p>

<p>To answer this question, we developed a <i>double-decision making paradigm</i> in which subjects decide on the direction of <i>motion</i> as well as the <i>color</i> of a patch of dynamic dots. In the example below, motion is towards the right, and the dots are in majority yellow, so the subject would respond by making an eye-movement to the right-yellow target. We find that the sensory motion and color information can be acquired in <i>parallel</i> by the visual system. However, subjects rely on a <i>serial</i> process to combine information about the two features into a single decision.  </p>

<p><br /></p>

<p><img src="/images/DoubleDecision.png" alt="Double decision task" width="50%" /></p>

<p><br /></p>

<p>We study how neurons in parietal cortex can selectively process the motion and color information from distant specialized sensory areas. We use single-unit recordings and <a href="https://www.neuropixels.org/">high-channel count</a> electrophysiological measurements in monkeys to study how information about the motion and color decisions get combined into a single choice at the level of individual neurons and large neuronal populations.</p>

<h4 id="learn-more">Learn more</h4>

<p><a href="https://www.youtube.com/watch?v=K1LOm92KxUE">This 2 minute video</a> explains what we learned by studying the decisions of human participants.</p>

<h4 id="representative-work">Representative work</h4>

<ul>
  <li>
    <p><u>D. Jeurissen</u>*, A. Löffler*, Y.H.R. Kang*, A. Zylberberg^, D.M. Wolpert^, and M.N. Shadlen^ (2021). Serial time-multiplexed incorporation of evidence to make two decisions about one object. Talk at the Virtual Cosyne 2021 Conference, February 24 - 26. */^ Equal contribution. <a href="https://youtu.be/hlmezZErgd8">15 Minute talk available on YouTube</a>.</p>
  </li>
  <li>
    <p>Y.H.R. Kang*, A. Löffler*, <u>D. Jeurissen</u>*, A. Zylberberg^, D.M. Wolpert^, M.N. Shadlen^ (2021). Multiple decisions about one object involve parallel sensory acquisition but time-multiplexed evidence incorporation. eLife, 10, e63721 <i class="fa fa-file-pdf-o"></i> <a href="../publications/papers/2021_KangLofflerJeurissen_eLife.pdf">PDF</a>. <a href="https://docs.google.com/presentation/d/1C-pOKoxp6eshfWkr_SGmX9JafvB0Jfuu_RADzy_yX7U/edit?usp=sharing">Journal Club Slides (Google doc)</a>
<br /> */^ Equal contribution</p>
  </li>
</ul>

<hr />

<h2 id="causal-manipulation-of-neural-activity">Causal manipulation of neural activity</h2>

<p>If a brain area can (temporarily) not provide useful information — due to a stroke, a traumatic brain injury, or experimental manipulation — can other brain areas then compensate for the lost function?<br /><br /></p>

<p>To answer this question, we use <i>optogenetic, chemogenetic, and pharmacological tools</i> to manipulate neural activity while monkeys make complex decisions. Across three different tasks, and using any of the three inactivation techniques, we find that manipulation of neural activity changes the animal’s choices. Surprisingly, the effects are only transient: behavioral effects fade away, even though neural activity is still suppressed. These results show that neurons in distant brain areas can compensate for lost functions.<br /><br /></p>

<p><img src="/images/Jaws.png" alt="Fig2B eLife paper showing Jaws expression" width="50%" /></p>

<p><br /></p>

<p>Our current research aims to reveal how neurons in distant brain areas change their activity pattern to compensate for the lost function of the inactivated region.</p>

<h4 id="representative-work-1">Representative work</h4>

<ul>
  <li>
    <p><u>D. Jeurissen</u><strong>*</strong>, S. Shushruth<strong>*</strong>, Y. El-Shamayleh, G.D. Horwitz, M.N. Shadlen (2022). Deficits in decision-making induced by parietal cortex inactivation are compensated at two time scales. Neuron, 110, 1-8. (Similar to: <a href="https://www.biorxiv.org/content/10.1101/2021.09.10.459856v1">BioRxiv</a>.) * Equal contribution. <i class="fa fa-file-pdf-o"></i> <a href="https://www.biorxiv.org/content/10.1101/2021.09.10.459856v1.full.pdf">PDF</a></p>
  </li>
  <li>
    <p>C.R. Fetsch, N.N. Odean, <u>D. Jeurissen</u>, Y. El-Shamayleh, G.D. Horwitz, M.N. Shadlen (2018). Focal optogenetic suppression in macaque area MT biases direction discrimination and choice confidence, but only transiently. eLife,  7, 36523. <i class="fa fa-file-pdf-o"></i> <a href="https://elifesciences.org/download/aHR0cHM6Ly9jZG4uZWxpZmVzY2llbmNlcy5vcmcvYXJ0aWNsZXMvMzY1MjMvZWxpZmUtMzY1MjMtdjIucGRm/elife-36523-v2.pdf?_hash=IcM8Uw4KaAGU0FT9Uws4THeYryZBJhJcxCYFamXP618%3D">PDF</a></p>
  </li>
</ul>

<hr />

<h2 id="attention-learning-and-object-perception">Attention, learning, and object perception</h2>

<p><br /></p>

<p><img src="/images/PerceptualGrouping.png" alt="Perceptual grouping of natural image" width="95%" /></p>

<p><br /></p>

<p>Each cell in the retina responds to light from only a tiny fragment of a visual scene. The fragmented input from the retina to the rest of the visual system has to be transformed into a representation of objects in front of a background, a process termed <i>perceptual grouping</i>. My graduate work at the Netherlands Institute for Neuroscience focused on how the primate visual system can achieve perceptual grouping of complex objects.<br /><br /></p>

<p><img src="/images/NorU.png" alt="Fig2A Curr Bio paper showing figure-ground modulation" width="50%" /></p>

<p>The fragment of the visual scene to which a cell responds is called its <i>receptive field</i>. Neurons in early visual areas show an increased response when their receptive field lies on a figure compared to on a background. This increased response is known as <i>figure-ground modulation</i>. Our experiments revealed that <i>attention</i> and <i>learning</i> selectively change the <i>enhancement</i> and <i>suppression</i> on neural signals, and thereby change the figure-ground modulation of complex shapes.</p>

<p><img src="/images/GrowthCones.png" alt="Fig2C eLife paper showing growth-cone model" width="50%" /></p>

<p>The findings from our work in the macaque monkey are relevant for our understanding of the human visual system. In behavioral experiments with humans we showed that perceptual grouping is achieved by spreading object-based attention over the surface of an object. Our neurocomputational <i>growth-cone model</i> — which is inspired by the results from monkey electrophysiology — accurately explains human behavior in a perceptual grouping task.</p>

<h4 id="representative-work-2">Representative work</h4>

<ul>
  <li>
    <p>M.W. Self*, <u>D. Jeurissen</u>*, A.F. van Ham, B. van Vugt, J. Poort, and P.R. Roelfsema (2019). The segmentation of proto-objects in the monkey primary visual cortex. Current Biology, 29, 1019-1029. <i class="fa fa-file-pdf-o"></i> <a href="papers/2019_SelfJeurissen_CurrBio.pdf">PDF</a> <a href="https://docs.google.com/presentation/d/1NP975PPtftqyhtHeBIvX3eqrt09Baosk2KnYj7ga-M8/edit?usp=sharing">Journal Club Slides (Google doc)</a>
<br /> * Equal contribution</p>
  </li>
  <li>
    <p><u>D. Jeurissen</u>, M.W. Self, and P.R. Roelfsema (2016). Serial grouping of 2D-image regions with object-based attention in humans. eLife,  5, e14320. <i class="fa fa-file-pdf-o"></i> <a href="https://elifesciences.org/content/5/e14320-download.pdf">PDF</a> <a href="https://docs.google.com/presentation/d/1P0uZVKC5OgPQ06t2YSn6-mA7p63dNiBeTSKkxvJFS8c/edit?usp=sharing">Journal Club Slides (Google doc)</a></p>
  </li>
</ul>

<hr />

<h2 id="collaborators">Collaborators</h2>

<ul>
  <li><a href="http://www.yasmineshamayleh.com/">Yasmine El-Shamayleh, Columbia University</a></li>
  <li><a href="https://www.fetschlab.org/">Chris Fetsch, Johns Hopkins University</a></li>
  <li><a href="http://faculty.washington.edu/ghorwitz/wordpress/">Greg Horwitz, University of Washington</a></li>
  <li><a href="https://www.tsaolab.caltech.edu/">Doris Tsao, California Institute of Technology</a></li>
  <li><a href="https://wolpertlab.neuroscience.columbia.edu/">Daniel Wolpert, Columbia University</a></li>
  <li><a href="https://www.zylberberg.org/">Ariel Zylberberg, University of Rochester</a></li>
</ul>

<hr />

<h2 id="publications">Publications</h2>

<p><a href="/publications/">Click here</a> for a full list of my publications, including papers that are not highlighted on this page.</p>

    </div><!-- /.article-wrap -->
    
  </article>
</div><!-- /#index -->

<div class="footer-wrap">
  <footer>
    

<span>&copy; 2022 Danique Jeurissen.

  </footer>
</div><!-- /.footer-wrap -->

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="https://daniquejeurissen.github.io/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="https://daniquejeurissen.github.io/assets/js/scripts.min.js"></script>

<!-- Asynchronous Google Analytics snippet -->
<script>
  var _gaq = _gaq || [];
  var pluginUrl = 
 '//www.google-analytics.com/plugins/ga/inpage_linkid.js';
  _gaq.push(['_require', 'inpage_linkid', pluginUrl]);
  _gaq.push(['_setAccount', 'UA-84651979-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>

          

</body>
</html>